{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 从语言模型到循环神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H_t的形状：torch.Size([2, 4])，O_t的形状：torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "## 计算 Ht，假设激活函数为 ReLU。\n",
    "X, W_xh = torch.normal( 0, 1, (2,3) ), torch.normal( 0, 1, (3,4) )\n",
    "H, W_hh = torch.normal( 0, 1, (2,4) ), torch.normal( 0, 1, (4,4) )\n",
    "B_h =  torch.normal( 0, 1, (1,4) )\n",
    "\n",
    "# H1 = torch.matmul(X, W_xh) + torch.matmul(H, W_hh) + B_h\n",
    "H1 = X @ W_xh + H @ W_hh + B_h\n",
    "H_t = F.relu(H1)\n",
    "\n",
    "##计算 O_t，输出激活函数为 softmax\n",
    "W_hm = torch.normal( 0, 1,(4, 2) )\n",
    "B_m = torch.normal( 0, 1,(1, 2) )\n",
    "\n",
    "# O = torch.matmul(H_t, W_hm) + B_m\n",
    "O = H_t @ W_hm + B_m\n",
    "O_t = F.softmax(O, dim=-1)\n",
    "print(\"H_t的形状：{}，O_t的形状：{}\".format(H_t.shape,O_t.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------矩阵H_t------------------------------\n",
      "tensor([[1.1417, 0.0000, 3.1171, 0.0000],\n",
      "        [3.2871, 0.0000, 2.5014, 0.0000]])\n",
      "------------------------------矩阵H02------------------------------\n",
      "tensor([[1.1417, 0.0000, 3.1171, 0.0000],\n",
      "        [3.2871, 0.0000, 2.5014, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "X_a = torch.cat((X, H), dim=1)\n",
    "W_a = torch.cat((W_xh, W_hh), dim=0)\n",
    "\n",
    "H01 = X_a @ W_a + B_h\n",
    "H02 = F.relu(H01)\n",
    "\n",
    "###查看矩阵H_t和H02\n",
    "print(\"-\"*30+\"矩阵H_t\"+\"-\"*30)\n",
    "print(H_t)\n",
    "print(\"-\"*30+\"矩阵H02\"+\"-\"*30)\n",
    "print(H02)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2前向传播与随时间反向传播"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./image1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状态值_0:  [0.53704957 0.46211716]\n",
      "输出值_0:  [1.56128388]\n",
      "状态值_1:  [0.85973818 0.88366641]\n",
      "输出值_1:  [2.72707101]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = [1, 2]\n",
    "state = [0.0, 0.0]\n",
    "\n",
    "w_cell_state = np.asarray([ [0.1, 0.2], [0.3, 0.4], [0.5, 0.6] ])\n",
    "b_cell = np.asarray([0.1, -0.1])\n",
    "\n",
    "w_output = np.asarray([[1.0], [2.0]])\n",
    "b_output = 0.1\n",
    "\n",
    "for i in range(len(X)):\n",
    "    state = np.append(state, X[i])\n",
    "    # before_activation = np.dot(state, w_cell_state) + b_cell\n",
    "    before_activation = state @ w_cell_state + b_cell\n",
    "    state = np.tanh(before_activation)\n",
    "    # final_output = np.dot(state, w_output) + b_output\n",
    "    final_output = state @ w_output + b_output\n",
    "    print(\"状态值_%i: \"%i, state)\n",
    "    print(\"输出值_%i: \"%i, final_output)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 循环神经网络的 Pytorch 实现\n",
    "前面我们介绍了循环神经网络的基本架构及其LSTM、GRU等变种。针对这些循环神经网络，PyTorch均提供了相应的API，如单元版的有nn.RNNCell、nn.LSTMCell、nn.GRUCell等，封装版的有nn.RNN、nn.LSTM、nn.GRU。单元版与封装版的最大区别在于输入，前者的输入是时间步或序列的一个元素，后者的输入是一个时间步序列。利用这些API可以极大提高我们的开发效率。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.1 使用PyTorch实现RNN\n",
    "\tPyTorch为RNN提供了两个版本的循环神经网络接口，单元版的输入是每个时间步或循环神经网络的一个循环，而封装版的输入是一个序列。下面我们从简单的封装版torch.nn.RNN开始，其一般格式为：\n",
    "torch.nn.RNN( args, * kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为使大家对循环神经网络有个直观理解，下面先用PyTorch实现简单循环神经网络，然后验证其关键要素。\n",
    "\t首先建立一个简单循环神经网络，输入维度为10，隐含状态维度为20，单向两层网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(input_size=10, hidden_size=20,num_layers= 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因输入节点与隐含层节点是全连接，根据输入维度、隐含层维度，可以推算出相关权重参数的维度，w_ih应该是20x10，w_hh是20x20, b_ih和b_hh都是hidden_size。下面我们通过查询weight_ih_l0、weight_hh_l0等进行验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wih形状torch.Size([20, 10]),whh形状torch.Size([20, 20]),bih形状torch.Size([20])\n",
      "wih形状torch.Size([20, 20]),whh形状torch.Size([20, 20]),bih形状torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "#第一层相关权重参数形状\n",
    "print(\"wih形状{},whh形状{},bih形状{}\".format(rnn.weight_ih_l0.shape,rnn.weight_hh_l0.shape,rnn.bias_hh_l0.shape))\n",
    "#wih形状torch.Size([20, 10]),whh形状torch.Size([20, 20]),bih形状#torch.Size([20])\n",
    "#第二层相关权重参数形状\n",
    "print(\"wih形状{},whh形状{},bih形状{}\".format(rnn.weight_ih_l1.shape,rnn.weight_hh_l1.shape,rnn.bias_hh_l1.shape))\n",
    "# wih形状torch.Size([20, 20]),whh形状torch.Size([20, 20]),bih形状#torch.Size([20])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN已搭建好，接下来将输入(x_t 、h_0)传入网络，根据网络配置及网络要求，生成输入数据。输入特征长度为100，批量大小为32，特征维度为10的张量。按网络要求，隐含状态的形状为(2,32,20)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成输入数据\n",
    "input=torch.randn(100,32,10)\n",
    "h_0=torch.randn(2,32,20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将输入数据传入RNN，将得到输出及更新后的隐含状态值。根据以上规则，输出output的形状应该是（100,32,20），隐含状态的输出的形状应该与输入的形状一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 32, 20]) torch.Size([2, 32, 20])\n"
     ]
    }
   ],
   "source": [
    "output,h_n=rnn(input,h_0)\n",
    "print(output.shape,h_n.shape)\n",
    "#torch.Size([100, 32, 20]) torch.Size([2, 32, 20])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果与我们设想的完全一致。\n",
    "\tRNNCell的输入的形状是(batch,input_size)，没有序列长度，这是因为隐含状态的输入只有单层，故其形状为(batch,hdden_size)。网络的输出只有隐含状态输出，其形状与输入一致，即(batch,hdden_size)。\n",
    "\t接下来我们利用PyTorch实现RNN，RNN由全连接层来构建，每一步输出预测和隐含状态，先前的隐含状态输入至下一时刻，具体如图7-12所示。\n",
    "![image.png](./image2.png)\n",
    "<center>图7-12 RNN架构</center>\n",
    "图7-12所示架构是一个典型的RNN架构，我们将用PyTorch实现该网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), dim = 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "n_hidden = 128\n",
    "#rnn = RNN(n_letters, n_hidden, n_categories)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.2 使用PyTorch实现LSTM\n",
    "\tLSTM是在RNN基础上增加了长时间记忆功能，具体通过增加一个状态C及3个门实现对信息的更精准控制。具体实现可参考图7-9。\n",
    "![image.png](./image3.png)\n",
    "<center>图7-9  LSTM 架构图</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, cell_size, output_size):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell_size = cell_size\n",
    "        self.gate = nn.Linear(input_size + hidden_size, cell_size)\n",
    "        self.output = nn.Linear(hidden_size, cell_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def farword(self, input, hidden, cell):\n",
    "        combined = torch.cat((input, hidden), dim = 1)\n",
    "        f_gate = self.sigmoid(self.gate(combined))\n",
    "        i_gate = self.sigmoid(self.gate(combined))\n",
    "        o_gate = self.sigmoid(self.gate(combined))\n",
    "        z_state = self.tanh(self.gate(combined))\n",
    "        cell = torch.add(torch.mul(cell, f_gate), torch.mul(z_state, i_gate))\n",
    "        hidden = torch.mul(self.tanh(cell), o_gate)\n",
    "        output = self.output(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden, cell\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "    def initCell(self):\n",
    "        return torch.zeros(1, self.cell_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.3 使用PyTorch实现GRU\n",
    "从图7-10可知，GRU架构与LSTM架构基本相同，主要区别是LSTM共有3个门，2个隐含状态；而GRU只有2个门，1个隐含状态。GRU的参数个数是标准RNN的3倍。\n",
    " ![image.png](./image4.jpg)\n",
    " <center>图7-10 GRU架构，其中小圆圈表示向量的点积</center>\n",
    "用PyTorch实现GRU单元与实现LSTM单元的方法类似，具体代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GRUCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), dim = 1) \n",
    "        z_gate = self.sigmoid(self.gate(combined))\n",
    "        r_gate = self.sigmoid(self.gate(combined))\n",
    "        combined01 = torch.cat((input, torch.mul(hidden,r_gate)), dim = 1) \n",
    "        h1_state = self.tanh(self.gate(combined01))\n",
    "\n",
    "        h_state = torch.add(torch.mul((1-z_gate), hidden), torch.mul(h1_state, z_gate))\n",
    "        output = self.output(h_state)\n",
    "        output = self.softmax(output)\n",
    "        return output, h_state\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "grucell = GRUCell(input_size=10,hidden_size=20,output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "grucell = GRUCell(input_size=10,hidden_size=20,output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10]) torch.Size([32, 20])\n"
     ]
    }
   ],
   "source": [
    "input=torch.randn(32,10)\n",
    "h_0=torch.randn(32,20)\n",
    "\n",
    "output,hn=grucell(input,h_0)\n",
    "print(output.size(),hn.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
