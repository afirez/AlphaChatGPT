{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "\n",
    "from config import MixtralConfig\n",
    "\n",
    "# Mistral 中 MistralMLP，与 Mixtral 中 MixtralBLockSparseTop2MLP 模型结构相似\n",
    "class MistralMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
    "        self.act_fn = ACT2FN[config.hidden_act]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "class MixtralBLockSparseTop2MLP(nn.Module):\n",
    "    def __init__(self, config: MixtralConfig):\n",
    "        super().__init__()\n",
    "        self.ffn_dim = config.intermediate_size\n",
    "        self.hidden_dim = config.hidden_size\n",
    "\n",
    "        self.w1 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)\n",
    "        self.w2 = nn.Linear(self.ffn_dim, self.hidden_dim, bias=False)\n",
    "        self.w3 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)\n",
    "\n",
    "        self.act_fn = ACT2FN[config.hidden_act]\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        current_hidden_states = self.act_fn(self.w1(hidden_states)) * self.w3(hidden_states)\n",
    "        current_hidden_states = self.w2(current_hidden_states)\n",
    "        return current_hidden_states\n",
    "    \n",
    "class MixtralSparseMoeBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    This implementation is\n",
    "    strictly equivalent to standard MoE with full capacity (no\n",
    "    dropped tokens). It's faster since it formulates MoE operations\n",
    "    in terms of block-sparse operations to accomodate imbalanced\n",
    "    assignments of tokens to experts, whereas standard MoE either\n",
    "    (1) drop tokens at the cost of reduced performance or (2) set\n",
    "    capacity factor to number of experts and thus waste computation\n",
    "    and memory on padding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = config.hidden_size\n",
    "        self.ffn_dim = config.intermediate_size\n",
    "        self.num_experts = config.num_local_experts\n",
    "        self.top_k = config.num_experts_per_tok\n",
    "\n",
    "        # gating\n",
    "        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)\n",
    "\n",
    "        self.experts = nn.ModuleList([MixtralBLockSparseTop2MLP(config) for _ in range(self.num_experts)])\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" \"\"\"\n",
    "        batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "        hidden_states = hidden_states.view(-1, hidden_dim)\n",
    "        # router_logits: (batch * sequence_length, n_experts)\n",
    "        router_logits = self.gate(hidden_states)\n",
    "\n",
    "        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n",
    "        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n",
    "        # we cast back to the input dtype\n",
    "        routing_weights = routing_weights.to(hidden_states.dtype)\n",
    "\n",
    "        final_hidden_states = torch.zeros(\n",
    "            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device\n",
    "        )\n",
    "\n",
    "        # One hot encode the selected experts to create an expert mask\n",
    "        # this will be used to easily index which expert is going to be sollicitated\n",
    "        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n",
    "\n",
    "        # Loop over all available experts in the model and perform the computation on each expert\n",
    "        for expert_idx in range(self.num_experts):\n",
    "            expert_layer = self.experts[expert_idx]\n",
    "            idx, top_x = torch.where(expert_mask[expert_idx])\n",
    "\n",
    "            if top_x.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            # in torch it is faster to index using lists than torch tensors\n",
    "            top_x_list = top_x.tolist()\n",
    "            idx_list = idx.tolist()\n",
    "\n",
    "            # Index the correct hidden states and compute the expert hidden state for\n",
    "            # the current expert. We need to make sure to multiply the output hidden\n",
    "            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n",
    "            current_state = hidden_states[None, top_x_list].reshape(-1, hidden_dim)\n",
    "            current_hidden_states = expert_layer(current_state) * routing_weights[top_x_list, idx_list, None]\n",
    "\n",
    "            # However `index_add_` only support torch tensors for indexing so we'll use\n",
    "            # the `top_x` tensor here.\n",
    "            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n",
    "        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "        return final_hidden_states, router_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixtralConfig = MixtralConfig(\n",
    "    vocab_size=3200, hidden_size=2048, intermediate_size=14336 // 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixtralSparseMoeBlock = MixtralSparseMoeBlock(mixtralConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, sequence_length, hidden_dim =  10, 50, 2048\n",
    "\n",
    "hidden_states = torch.randn(size=(batch_size, sequence_length, hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hidden_states, router_logits = mixtralSparseMoeBlock(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hidden_states.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
