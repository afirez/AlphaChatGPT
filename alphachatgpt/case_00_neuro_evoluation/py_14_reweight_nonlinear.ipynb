{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reweight GPT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reweight-GPT: An alternative to self-attetnion. \n",
    "\n",
    "This script is directly adapted from Andrej Karpathy's GPT project for easy comparsion.\n",
    "\n",
    "The self-attention parts are replaced with direct re-weighting mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "# with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    # text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "\n",
    "hidden_nodes = 20 # this parameter controls the number of the middle nodes for self.wr\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe5d566fdb0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '-',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '9',\n",
       " ':',\n",
       " '[',\n",
       " ']',\n",
       " '。',\n",
       " '一',\n",
       " '七',\n",
       " '万',\n",
       " '三',\n",
       " '上',\n",
       " '下',\n",
       " '不',\n",
       " '专',\n",
       " '且',\n",
       " '世',\n",
       " '业',\n",
       " '东',\n",
       " '个',\n",
       " '中',\n",
       " '丸',\n",
       " '为',\n",
       " '么',\n",
       " '之',\n",
       " '乐',\n",
       " '也',\n",
       " '买',\n",
       " '了',\n",
       " '事',\n",
       " '二',\n",
       " '于',\n",
       " '五',\n",
       " '些',\n",
       " '交',\n",
       " '产',\n",
       " '人',\n",
       " '什',\n",
       " '仅',\n",
       " '今',\n",
       " '介',\n",
       " '从',\n",
       " '仓',\n",
       " '他',\n",
       " '以',\n",
       " '们',\n",
       " '件',\n",
       " '价',\n",
       " '任',\n",
       " '份',\n",
       " '会',\n",
       " '传',\n",
       " '伦',\n",
       " '伸',\n",
       " '伺',\n",
       " '但',\n",
       " '位',\n",
       " '低',\n",
       " '体',\n",
       " '何',\n",
       " '作',\n",
       " '你',\n",
       " '佣',\n",
       " '俗',\n",
       " '保',\n",
       " '信',\n",
       " '倒',\n",
       " '候',\n",
       " '借',\n",
       " '值',\n",
       " '假',\n",
       " '做',\n",
       " '像',\n",
       " '儿',\n",
       " '充',\n",
       " '先',\n",
       " '党',\n",
       " '兜',\n",
       " '入',\n",
       " '全',\n",
       " '八',\n",
       " '六',\n",
       " '关',\n",
       " '其',\n",
       " '具',\n",
       " '内',\n",
       " '再',\n",
       " '写',\n",
       " '冲',\n",
       " '净',\n",
       " '凑',\n",
       " '几',\n",
       " '出',\n",
       " '分',\n",
       " '切',\n",
       " '列',\n",
       " '则',\n",
       " '创',\n",
       " '初',\n",
       " '利',\n",
       " '别',\n",
       " '到',\n",
       " '制',\n",
       " '刺',\n",
       " '刻',\n",
       " '前',\n",
       " '力',\n",
       " '功',\n",
       " '加',\n",
       " '动',\n",
       " '势',\n",
       " '包',\n",
       " '区',\n",
       " '十',\n",
       " '千',\n",
       " '升',\n",
       " '半',\n",
       " '单',\n",
       " '卖',\n",
       " '历',\n",
       " '原',\n",
       " '去',\n",
       " '又',\n",
       " '及',\n",
       " '反',\n",
       " '发',\n",
       " '取',\n",
       " '句',\n",
       " '另',\n",
       " '只',\n",
       " '可',\n",
       " '台',\n",
       " '史',\n",
       " '号',\n",
       " '吃',\n",
       " '各',\n",
       " '合',\n",
       " '同',\n",
       " '后',\n",
       " '吧',\n",
       " '听',\n",
       " '呀',\n",
       " '告',\n",
       " '呢',\n",
       " '和',\n",
       " '品',\n",
       " '哪',\n",
       " '哭',\n",
       " '售',\n",
       " '唯',\n",
       " '商',\n",
       " '嘛',\n",
       " '回',\n",
       " '因',\n",
       " '国',\n",
       " '图',\n",
       " '圈',\n",
       " '在',\n",
       " '地',\n",
       " '场',\n",
       " '坐',\n",
       " '块',\n",
       " '型',\n",
       " '堆',\n",
       " '境',\n",
       " '声',\n",
       " '处',\n",
       " '复',\n",
       " '外',\n",
       " '多',\n",
       " '夜',\n",
       " '够',\n",
       " '大',\n",
       " '天',\n",
       " '夫',\n",
       " '失',\n",
       " '头',\n",
       " '套',\n",
       " '奶',\n",
       " '好',\n",
       " '如',\n",
       " '妇',\n",
       " '姑',\n",
       " '姨',\n",
       " '娱',\n",
       " '娶',\n",
       " '媒',\n",
       " '媳',\n",
       " '子',\n",
       " '存',\n",
       " '完',\n",
       " '官',\n",
       " '定',\n",
       " '实',\n",
       " '客',\n",
       " '家',\n",
       " '察',\n",
       " '对',\n",
       " '小',\n",
       " '就',\n",
       " '巧',\n",
       " '差',\n",
       " '己',\n",
       " '已',\n",
       " '巴',\n",
       " '币',\n",
       " '市',\n",
       " '常',\n",
       " '幅',\n",
       " '干',\n",
       " '平',\n",
       " '年',\n",
       " '并',\n",
       " '广',\n",
       " '应',\n",
       " '底',\n",
       " '度',\n",
       " '座',\n",
       " '庭',\n",
       " '延',\n",
       " '开',\n",
       " '式',\n",
       " '张',\n",
       " '归',\n",
       " '当',\n",
       " '形',\n",
       " '彼',\n",
       " '待',\n",
       " '很',\n",
       " '得',\n",
       " '心',\n",
       " '必',\n",
       " '忆',\n",
       " '态',\n",
       " '怎',\n",
       " '思',\n",
       " '急',\n",
       " '总',\n",
       " '息',\n",
       " '情',\n",
       " '想',\n",
       " '意',\n",
       " '成',\n",
       " '我',\n",
       " '户',\n",
       " '房',\n",
       " '所',\n",
       " '手',\n",
       " '才',\n",
       " '打',\n",
       " '扣',\n",
       " '找',\n",
       " '承',\n",
       " '把',\n",
       " '投',\n",
       " '折',\n",
       " '报',\n",
       " '抹',\n",
       " '押',\n",
       " '担',\n",
       " '拆',\n",
       " '拉',\n",
       " '拣',\n",
       " '择',\n",
       " '括',\n",
       " '拿',\n",
       " '持',\n",
       " '挂',\n",
       " '按',\n",
       " '挑',\n",
       " '换',\n",
       " '据',\n",
       " '掉',\n",
       " '接',\n",
       " '控',\n",
       " '提',\n",
       " '搬',\n",
       " '操',\n",
       " '收',\n",
       " '放',\n",
       " '教',\n",
       " '敦',\n",
       " '数',\n",
       " '整',\n",
       " '文',\n",
       " '新',\n",
       " '方',\n",
       " '旅',\n",
       " '无',\n",
       " '既',\n",
       " '日',\n",
       " '旧',\n",
       " '早',\n",
       " '时',\n",
       " '明',\n",
       " '易',\n",
       " '是',\n",
       " '普',\n",
       " '最',\n",
       " '月',\n",
       " '有',\n",
       " '期',\n",
       " '未',\n",
       " '本',\n",
       " '来',\n",
       " '松',\n",
       " '果',\n",
       " '某',\n",
       " '标',\n",
       " '树',\n",
       " '样',\n",
       " '格',\n",
       " '楚',\n",
       " '楼',\n",
       " '模',\n",
       " '次',\n",
       " '此',\n",
       " '步',\n",
       " '殊',\n",
       " '段',\n",
       " '每',\n",
       " '比',\n",
       " '毛',\n",
       " '民',\n",
       " '水',\n",
       " '求',\n",
       " '汇',\n",
       " '江',\n",
       " '没',\n",
       " '法',\n",
       " '泣',\n",
       " '注',\n",
       " '流',\n",
       " '浙',\n",
       " '海',\n",
       " '消',\n",
       " '润',\n",
       " '涨',\n",
       " '清',\n",
       " '游',\n",
       " '源',\n",
       " '漏',\n",
       " '澳',\n",
       " '炒',\n",
       " '点',\n",
       " '热',\n",
       " '然',\n",
       " '照',\n",
       " '熟',\n",
       " '爆',\n",
       " '片',\n",
       " '牌',\n",
       " '牛',\n",
       " '牵',\n",
       " '特',\n",
       " '狼',\n",
       " '猛',\n",
       " '玩',\n",
       " '现',\n",
       " '理',\n",
       " '琢',\n",
       " '生',\n",
       " '用',\n",
       " '白',\n",
       " '百',\n",
       " '的',\n",
       " '盈',\n",
       " '益',\n",
       " '目',\n",
       " '相',\n",
       " '看',\n",
       " '真',\n",
       " '着',\n",
       " '知',\n",
       " '短',\n",
       " '矿',\n",
       " '研',\n",
       " '确',\n",
       " '碧',\n",
       " '磨',\n",
       " '票',\n",
       " '禁',\n",
       " '种',\n",
       " '称',\n",
       " '程',\n",
       " '究',\n",
       " '空',\n",
       " '立',\n",
       " '章',\n",
       " '第',\n",
       " '等',\n",
       " '答',\n",
       " '简',\n",
       " '管',\n",
       " '类',\n",
       " '系',\n",
       " '组',\n",
       " '经',\n",
       " '绕',\n",
       " '给',\n",
       " '绝',\n",
       " '续',\n",
       " '网',\n",
       " '罗',\n",
       " '置',\n",
       " '羊',\n",
       " '美',\n",
       " '翻',\n",
       " '老',\n",
       " '考',\n",
       " '者',\n",
       " '而',\n",
       " '聊',\n",
       " '职',\n",
       " '股',\n",
       " '育',\n",
       " '能',\n",
       " '自',\n",
       " '花',\n",
       " '草',\n",
       " '荷',\n",
       " '行',\n",
       " '街',\n",
       " '补',\n",
       " '表',\n",
       " '衷',\n",
       " '被',\n",
       " '西',\n",
       " '要',\n",
       " '观',\n",
       " '觉',\n",
       " '解',\n",
       " '记',\n",
       " '讲',\n",
       " '许',\n",
       " '论',\n",
       " '证',\n",
       " '诉',\n",
       " '话',\n",
       " '该',\n",
       " '说',\n",
       " '读',\n",
       " '谁',\n",
       " '谈',\n",
       " '谓',\n",
       " '谱',\n",
       " '象',\n",
       " '质',\n",
       " '购',\n",
       " '贴',\n",
       " '费',\n",
       " '资',\n",
       " '赌',\n",
       " '赚',\n",
       " '走',\n",
       " '起',\n",
       " '跌',\n",
       " '跑',\n",
       " '跟',\n",
       " '路',\n",
       " '车',\n",
       " '转',\n",
       " '载',\n",
       " '迅',\n",
       " '过',\n",
       " '运',\n",
       " '近',\n",
       " '还',\n",
       " '这',\n",
       " '进',\n",
       " '连',\n",
       " '迟',\n",
       " '追',\n",
       " '选',\n",
       " '逐',\n",
       " '通',\n",
       " '速',\n",
       " '遇',\n",
       " '遍',\n",
       " '道',\n",
       " '遥',\n",
       " '那',\n",
       " '部',\n",
       " '都',\n",
       " '采',\n",
       " '里',\n",
       " '重',\n",
       " '量',\n",
       " '金',\n",
       " '钱',\n",
       " '链',\n",
       " '锁',\n",
       " '错',\n",
       " '长',\n",
       " '门',\n",
       " '问',\n",
       " '间',\n",
       " '闻',\n",
       " '际',\n",
       " '限',\n",
       " '随',\n",
       " '隔',\n",
       " '零',\n",
       " '雾',\n",
       " '需',\n",
       " '非',\n",
       " '靠',\n",
       " '面',\n",
       " '须',\n",
       " '题',\n",
       " '额',\n",
       " '风',\n",
       " '高',\n",
       " '黄',\n",
       " '鼻',\n",
       " '，',\n",
       " '？']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 494,\n",
       " 352,\n",
       " 49,\n",
       " 312,\n",
       " 21,\n",
       " 304,\n",
       " 79,\n",
       " 521,\n",
       " 496,\n",
       " 523,\n",
       " 419,\n",
       " 304,\n",
       " 154,\n",
       " 390,\n",
       " 79,\n",
       " 521,\n",
       " 496,\n",
       " 524,\n",
       " 0,\n",
       " 0,\n",
       " 128,\n",
       " 105,\n",
       " 1,\n",
       " 386,\n",
       " 317,\n",
       " 435,\n",
       " 519,\n",
       " 1,\n",
       " 440,\n",
       " 240,\n",
       " 260,\n",
       " 470,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 10,\n",
       " 2,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 11,\n",
       " 6,\n",
       " 3,\n",
       " 1,\n",
       " 133,\n",
       " 432,\n",
       " 39,\n",
       " 1,\n",
       " 341,\n",
       " 335,\n",
       " 0,\n",
       " 0,\n",
       " 251,\n",
       " 53,\n",
       " 308,\n",
       " 27,\n",
       " 449,\n",
       " 418,\n",
       " 523,\n",
       " 478,\n",
       " 409,\n",
       " 503,\n",
       " 36,\n",
       " 40,\n",
       " 89,\n",
       " 323,\n",
       " 523,\n",
       " 503,\n",
       " 521,\n",
       " 496,\n",
       " 14,\n",
       " 0,\n",
       " 51,\n",
       " 30,\n",
       " 45,\n",
       " 31,\n",
       " 503,\n",
       " 521,\n",
       " 496,\n",
       " 151,\n",
       " 524,\n",
       " 161,\n",
       " 30,\n",
       " 306,\n",
       " 474,\n",
       " 476,\n",
       " 327,\n",
       " 301,\n",
       " 504,\n",
       " 236,\n",
       " 177,\n",
       " 425,\n",
       " 194,\n",
       " 66,\n",
       " 165,\n",
       " 420,\n",
       " 521,\n",
       " 496,\n",
       " 523,\n",
       " 385,\n",
       " 102,\n",
       " 166,\n",
       " 448,\n",
       " 523,\n",
       " 304,\n",
       " 420,\n",
       " 44,\n",
       " 331,\n",
       " 213,\n",
       " 521,\n",
       " 496,\n",
       " 345,\n",
       " 118,\n",
       " 471,\n",
       " 364,\n",
       " 14,\n",
       " 0,\n",
       " 251,\n",
       " 378,\n",
       " 36,\n",
       " 20,\n",
       " 146,\n",
       " 139,\n",
       " 290,\n",
       " 279,\n",
       " 523,\n",
       " 476,\n",
       " 21,\n",
       " 304,\n",
       " 27,\n",
       " 215,\n",
       " 449,\n",
       " 370,\n",
       " 252,\n",
       " 523,\n",
       " 91,\n",
       " 201,\n",
       " 308,\n",
       " 301,\n",
       " 75,\n",
       " 242,\n",
       " 31,\n",
       " 446,\n",
       " 448,\n",
       " 151,\n",
       " 523,\n",
       " 69,\n",
       " 308,\n",
       " 115,\n",
       " 182,\n",
       " 328,\n",
       " 181,\n",
       " 503,\n",
       " 88,\n",
       " 160,\n",
       " 523,\n",
       " 21,\n",
       " 188,\n",
       " 20,\n",
       " 115,\n",
       " 182,\n",
       " 129,\n",
       " 261,\n",
       " 127,\n",
       " 140,\n",
       " 292,\n",
       " 396,\n",
       " 449,\n",
       " 15,\n",
       " 449,\n",
       " 14,\n",
       " 0,\n",
       " 188,\n",
       " 314,\n",
       " 69,\n",
       " 476,\n",
       " 27,\n",
       " 238,\n",
       " 243,\n",
       " 491,\n",
       " 21,\n",
       " 20,\n",
       " 523,\n",
       " 61,\n",
       " 255,\n",
       " 84,\n",
       " 79,\n",
       " 250,\n",
       " 476,\n",
       " 318,\n",
       " 523,\n",
       " 69,\n",
       " 385,\n",
       " 200,\n",
       " 69,\n",
       " 61,\n",
       " 255,\n",
       " 237,\n",
       " 109,\n",
       " 373,\n",
       " 26,\n",
       " 435,\n",
       " 523,\n",
       " 379,\n",
       " 373,\n",
       " 304,\n",
       " 425,\n",
       " 210,\n",
       " 512,\n",
       " 436,\n",
       " 373,\n",
       " 31,\n",
       " 524,\n",
       " 0,\n",
       " 44,\n",
       " 24,\n",
       " 504,\n",
       " 21,\n",
       " 197,\n",
       " 165,\n",
       " 180,\n",
       " 114,\n",
       " 29,\n",
       " 523,\n",
       " 475,\n",
       " 237,\n",
       " 304,\n",
       " 483,\n",
       " 370,\n",
       " 373,\n",
       " 523,\n",
       " 56,\n",
       " 450,\n",
       " 142,\n",
       " 36,\n",
       " 491,\n",
       " 187,\n",
       " 523,\n",
       " 24,\n",
       " 19,\n",
       " 336,\n",
       " 476,\n",
       " 365,\n",
       " 249,\n",
       " 81,\n",
       " 14,\n",
       " 0,\n",
       " 476,\n",
       " 27,\n",
       " 503,\n",
       " 517,\n",
       " 513,\n",
       " 215,\n",
       " 400,\n",
       " 125,\n",
       " 523,\n",
       " 494,\n",
       " 352,\n",
       " 49,\n",
       " 312,\n",
       " 21,\n",
       " 165,\n",
       " 39,\n",
       " 69,\n",
       " 503,\n",
       " 373,\n",
       " 79,\n",
       " 521,\n",
       " 496,\n",
       " 523,\n",
       " 419,\n",
       " 165,\n",
       " 39,\n",
       " 69,\n",
       " 304,\n",
       " 154,\n",
       " 15,\n",
       " 402,\n",
       " 79,\n",
       " 521,\n",
       " 496,\n",
       " 373,\n",
       " 524,\n",
       " 0,\n",
       " 165,\n",
       " 251,\n",
       " 378,\n",
       " 312,\n",
       " 523,\n",
       " 79,\n",
       " 521,\n",
       " 496,\n",
       " 308,\n",
       " 18,\n",
       " 402,\n",
       " 14,\n",
       " 0,\n",
       " 397,\n",
       " 15,\n",
       " 402,\n",
       " 304,\n",
       " 79,\n",
       " 330,\n",
       " 111,\n",
       " 185,\n",
       " 107,\n",
       " 373,\n",
       " 14,\n",
       " 0,\n",
       " 476,\n",
       " 390,\n",
       " 44,\n",
       " 152,\n",
       " 521,\n",
       " 21,\n",
       " 521,\n",
       " 496,\n",
       " 336,\n",
       " 90,\n",
       " 403,\n",
       " 523,\n",
       " 56,\n",
       " 67,\n",
       " 15,\n",
       " 27,\n",
       " 42,\n",
       " 303,\n",
       " 153,\n",
       " 390,\n",
       " 493,\n",
       " 491,\n",
       " 197,\n",
       " 165,\n",
       " 476,\n",
       " 390,\n",
       " 185,\n",
       " 107,\n",
       " 418,\n",
       " 14,\n",
       " 0,\n",
       " 69,\n",
       " 329,\n",
       " 188,\n",
       " 351,\n",
       " 253,\n",
       " 196,\n",
       " 373,\n",
       " 44,\n",
       " 523,\n",
       " 51,\n",
       " 485,\n",
       " 109,\n",
       " 362,\n",
       " 108,\n",
       " 65,\n",
       " 373,\n",
       " 126,\n",
       " 203,\n",
       " 523,\n",
       " 207,\n",
       " 471,\n",
       " 484,\n",
       " 35,\n",
       " 20,\n",
       " 312,\n",
       " 36,\n",
       " 523,\n",
       " 354,\n",
       " 146,\n",
       " 469,\n",
       " 255,\n",
       " 395,\n",
       " 112,\n",
       " 116,\n",
       " 55,\n",
       " 126,\n",
       " 100,\n",
       " 14,\n",
       " 476,\n",
       " 27,\n",
       " 116,\n",
       " 55,\n",
       " 373,\n",
       " 216,\n",
       " 224,\n",
       " 236,\n",
       " 206,\n",
       " 523,\n",
       " 72,\n",
       " 444,\n",
       " 51,\n",
       " 425,\n",
       " 210,\n",
       " 304,\n",
       " 65,\n",
       " 39,\n",
       " 214,\n",
       " 167,\n",
       " 379,\n",
       " 201,\n",
       " 250,\n",
       " 42,\n",
       " 55,\n",
       " 373,\n",
       " 14,\n",
       " 0,\n",
       " 51,\n",
       " 461,\n",
       " 373,\n",
       " 497,\n",
       " 236,\n",
       " 400,\n",
       " 125,\n",
       " 523,\n",
       " 207,\n",
       " 304,\n",
       " 15,\n",
       " 27,\n",
       " 69,\n",
       " 244,\n",
       " 373,\n",
       " 497,\n",
       " 14,\n",
       " 0,\n",
       " 91,\n",
       " 201,\n",
       " 69,\n",
       " 21,\n",
       " 244,\n",
       " 523,\n",
       " 69,\n",
       " 275,\n",
       " 5,\n",
       " 27,\n",
       " 307,\n",
       " 34,\n",
       " 424,\n",
       " 126,\n",
       " 280,\n",
       " 523,\n",
       " 448,\n",
       " 21,\n",
       " 200,\n",
       " 329,\n",
       " 51,\n",
       " 116,\n",
       " 55,\n",
       " 146,\n",
       " 373,\n",
       " 55,\n",
       " 319,\n",
       " 475,\n",
       " 520,\n",
       " 523,\n",
       " 63,\n",
       " 304,\n",
       " 161,\n",
       " 30,\n",
       " 362,\n",
       " 326,\n",
       " 128,\n",
       " 161,\n",
       " 523,\n",
       " 69,\n",
       " 239,\n",
       " 516,\n",
       " 6,\n",
       " 181,\n",
       " 93,\n",
       " 250,\n",
       " 42,\n",
       " 523,\n",
       " 489,\n",
       " 31,\n",
       " 69,\n",
       " 257,\n",
       " 36,\n",
       " 15,\n",
       " 27,\n",
       " 236,\n",
       " 180,\n",
       " 373,\n",
       " 263,\n",
       " 258,\n",
       " 523,\n",
       " 51,\n",
       " 207,\n",
       " 281,\n",
       " 36,\n",
       " 14,\n",
       " 354,\n",
       " 146,\n",
       " 51,\n",
       " 4,\n",
       " 27,\n",
       " 307,\n",
       " 93,\n",
       " 126,\n",
       " 280,\n",
       " 14,\n",
       " 0,\n",
       " 51,\n",
       " 207,\n",
       " 461,\n",
       " 15,\n",
       " 352,\n",
       " 352,\n",
       " 209,\n",
       " 55,\n",
       " 14,\n",
       " 0,\n",
       " 51,\n",
       " 424,\n",
       " 461,\n",
       " 476,\n",
       " 27,\n",
       " 209,\n",
       " 55,\n",
       " 523,\n",
       " 15,\n",
       " 294,\n",
       " 515,\n",
       " 51,\n",
       " 255,\n",
       " 493,\n",
       " 508,\n",
       " 301,\n",
       " 308,\n",
       " 489,\n",
       " 31,\n",
       " 177,\n",
       " 340,\n",
       " 117,\n",
       " 459,\n",
       " 496,\n",
       " 523,\n",
       " 136,\n",
       " 15,\n",
       " 294,\n",
       " 523,\n",
       " 51,\n",
       " 488,\n",
       " 282,\n",
       " 380,\n",
       " 180,\n",
       " 495,\n",
       " 373,\n",
       " 28,\n",
       " 48,\n",
       " 14,\n",
       " 0,\n",
       " 278,\n",
       " 135,\n",
       " 446,\n",
       " 448,\n",
       " 523,\n",
       " 397,\n",
       " 15,\n",
       " 73,\n",
       " 246,\n",
       " 15,\n",
       " 200,\n",
       " 304,\n",
       " 407,\n",
       " 109,\n",
       " 51,\n",
       " 373,\n",
       " 523,\n",
       " 397,\n",
       " 38,\n",
       " 73,\n",
       " 246,\n",
       " 256,\n",
       " 304,\n",
       " 165,\n",
       " 28,\n",
       " 48,\n",
       " 373,\n",
       " 206,\n",
       " 164,\n",
       " 196,\n",
       " 493,\n",
       " 340,\n",
       " 117,\n",
       " 523,\n",
       " 397,\n",
       " 18,\n",
       " 73,\n",
       " 246,\n",
       " 256,\n",
       " 58,\n",
       " 275,\n",
       " 359,\n",
       " 19,\n",
       " 410,\n",
       " 523,\n",
       " 434,\n",
       " 254,\n",
       " 308,\n",
       " 44,\n",
       " 378,\n",
       " 109,\n",
       " 14,\n",
       " 0,\n",
       " 421,\n",
       " 25,\n",
       " 351,\n",
       " 253,\n",
       " 202,\n",
       " 491,\n",
       " 304,\n",
       " 79,\n",
       " 397,\n",
       " 15,\n",
       " 73,\n",
       " 246,\n",
       " 369,\n",
       " 249,\n",
       " 373,\n",
       " 523,\n",
       " 278,\n",
       " 135,\n",
       " 446,\n",
       " 523,\n",
       " 69,\n",
       " 19,\n",
       " 410,\n",
       " 259,\n",
       " 109,\n",
       " 373,\n",
       " 253,\n",
       " 348,\n",
       " 523,\n",
       " 491,\n",
       " 304,\n",
       " 421,\n",
       " 25,\n",
       " 351,\n",
       " 253,\n",
       " 202,\n",
       " 21,\n",
       " 436,\n",
       " 373,\n",
       " 14,\n",
       " 277,\n",
       " 270,\n",
       " 32,\n",
       " 146,\n",
       " 523,\n",
       " 438,\n",
       " 237,\n",
       " 336,\n",
       " 308,\n",
       " 382,\n",
       " 309,\n",
       " 107,\n",
       " 375,\n",
       " 523,\n",
       " 256,\n",
       " 58,\n",
       " 434,\n",
       " 221,\n",
       " 180,\n",
       " 456,\n",
       " 253,\n",
       " 418,\n",
       " 378,\n",
       " 109,\n",
       " 14,\n",
       " 0,\n",
       " 69,\n",
       " 261,\n",
       " 476,\n",
       " 27,\n",
       " 322,\n",
       " 170,\n",
       " 287,\n",
       " 109,\n",
       " 254,\n",
       " 308,\n",
       " 42,\n",
       " 303,\n",
       " 153,\n",
       " 390,\n",
       " 19,\n",
       " 523,\n",
       " 491,\n",
       " 304,\n",
       " 15,\n",
       " 318,\n",
       " 373,\n",
       " 14,\n",
       " 0,\n",
       " 351,\n",
       " 422,\n",
       " 373,\n",
       " 523,\n",
       " 351,\n",
       " 334,\n",
       " 373,\n",
       " 523,\n",
       " 351,\n",
       " 158,\n",
       " 153,\n",
       " 373,\n",
       " 523,\n",
       " 351,\n",
       " 521,\n",
       " 496,\n",
       " 373,\n",
       " 523,\n",
       " 491,\n",
       " 304,\n",
       " 15,\n",
       " 318,\n",
       " 373,\n",
       " 14,\n",
       " 0,\n",
       " 188,\n",
       " 314,\n",
       " 69,\n",
       " 304,\n",
       " 476,\n",
       " 390,\n",
       " 44,\n",
       " 523,\n",
       " 489,\n",
       " 276,\n",
       " 355,\n",
       " 69,\n",
       " 425,\n",
       " 210,\n",
       " 373,\n",
       " 297,\n",
       " 308,\n",
       " 185,\n",
       " 467,\n",
       " 129,\n",
       " 79,\n",
       " 207,\n",
       " 429,\n",
       " 36,\n",
       " 149,\n",
       " 14,\n",
       " 0,\n",
       " 397,\n",
       " 38,\n",
       " 402,\n",
       " 304,\n",
       " 79,\n",
       " 205,\n",
       " 96,\n",
       " 185,\n",
       " 107,\n",
       " 373,\n",
       " 14,\n",
       " 0,\n",
       " 476,\n",
       " 390,\n",
       " 34,\n",
       " 304,\n",
       " 214,\n",
       " 167,\n",
       " 493,\n",
       " 305,\n",
       " 486,\n",
       " 197,\n",
       " 165,\n",
       " 373,\n",
       " 366,\n",
       " 454,\n",
       " 523,\n",
       " 119,\n",
       " 272,\n",
       " 477,\n",
       " 15,\n",
       " 325,\n",
       " 373,\n",
       " 227,\n",
       " 61,\n",
       " 233,\n",
       " 241,\n",
       " 491,\n",
       " 304,\n",
       " 308,\n",
       " 373,\n",
       " 14,\n",
       " 0,\n",
       " 69,\n",
       " 329,\n",
       " 188,\n",
       " 123,\n",
       " 332,\n",
       " 520,\n",
       " 373,\n",
       " 301,\n",
       " 75,\n",
       " 523,\n",
       " 308,\n",
       " 44,\n",
       " 207,\n",
       " 261,\n",
       " 162,\n",
       " 176,\n",
       " 373,\n",
       " 521,\n",
       " 496,\n",
       " 473,\n",
       " 477,\n",
       " 312,\n",
       " 523,\n",
       " 461,\n",
       " 476,\n",
       " 27,\n",
       " 209,\n",
       " 55,\n",
       " 14,\n",
       " 0,\n",
       " 132,\n",
       " 472,\n",
       " 312,\n",
       " 78,\n",
       " 188,\n",
       " 457,\n",
       " 332,\n",
       " 520,\n",
       " 523,\n",
       " 489,\n",
       " 31,\n",
       " 207,\n",
       " 308,\n",
       " 44,\n",
       " 74,\n",
       " 472,\n",
       " 312,\n",
       " 79,\n",
       " 14,\n",
       " 232,\n",
       " 354,\n",
       " 523,\n",
       " 74,\n",
       " 472,\n",
       " 312,\n",
       " 79,\n",
       " 308,\n",
       " 41,\n",
       " 507,\n",
       " 110,\n",
       " 523,\n",
       " 205,\n",
       " 147,\n",
       " 14,\n",
       " 92,\n",
       " 66,\n",
       " 251,\n",
       " 53,\n",
       " 21,\n",
       " 451,\n",
       " 523,\n",
       " 79,\n",
       " 373,\n",
       " 44,\n",
       " 381,\n",
       " 487,\n",
       " 251,\n",
       " 420,\n",
       " 45,\n",
       " 31,\n",
       " 14,\n",
       " 0,\n",
       " 489,\n",
       " 34,\n",
       " 308,\n",
       " 44,\n",
       " 80,\n",
       " 251,\n",
       " 32,\n",
       " 113,\n",
       " 420,\n",
       " 472,\n",
       " 373,\n",
       " 322,\n",
       " 229,\n",
       " 523,\n",
       " 51,\n",
       " 53,\n",
       " 79,\n",
       " 373,\n",
       " 201,\n",
       " 506,\n",
       " 19,\n",
       " 304,\n",
       " 160,\n",
       " 231,\n",
       " 523,\n",
       " 207,\n",
       " 304,\n",
       " 21,\n",
       " 401,\n",
       " 69,\n",
       " 123,\n",
       " 332,\n",
       " 457,\n",
       " 332,\n",
       " 523,\n",
       " 472,\n",
       " 327,\n",
       " 301,\n",
       " 504,\n",
       " 245,\n",
       " 58,\n",
       " 160,\n",
       " 231,\n",
       " 14,\n",
       " 0,\n",
       " 51,\n",
       " 53,\n",
       " 185,\n",
       " 134,\n",
       " 373,\n",
       " 304,\n",
       " 160,\n",
       " 231,\n",
       " 472,\n",
       " 392,\n",
       " 28,\n",
       " 373,\n",
       " 107,\n",
       " 344,\n",
       " 14,\n",
       " 0,\n",
       " 119,\n",
       " 272,\n",
       " 228,\n",
       " 492,\n",
       " 496,\n",
       " 383,\n",
       " 373,\n",
       " 158,\n",
       " 44,\n",
       " 523,\n",
       " 51,\n",
       " 53,\n",
       " 30,\n",
       " 36,\n",
       " 499,\n",
       " 200,\n",
       " 55,\n",
       " 319,\n",
       " 523,\n",
       " 425,\n",
       " 210,\n",
       " 398,\n",
       " 495,\n",
       " 79,\n",
       " 394,\n",
       " 523,\n",
       " 377,\n",
       " 232,\n",
       " 39,\n",
       " 283,\n",
       " 113,\n",
       " 261,\n",
       " 425,\n",
       " 210,\n",
       " 310,\n",
       " 312,\n",
       " 436,\n",
       " 492,\n",
       " 100,\n",
       " 312,\n",
       " 373,\n",
       " 496,\n",
       " 196,\n",
       " 83,\n",
       " 52,\n",
       " 15,\n",
       " 27,\n",
       " 385,\n",
       " 200,\n",
       " 373,\n",
       " 55,\n",
       " 319,\n",
       " 126,\n",
       " 280,\n",
       " 14,\n",
       " 0,\n",
       " 476,\n",
       " 207,\n",
       " 466,\n",
       " 315,\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0, 494, 352,  ...,  36,  14,   0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" self-attention  parts are commented. If you want to compare them with the direct reweighting uncomment them and remove wei=self.wr(x)\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        # self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.wr = nn.Sequential( nn.Linear(n_embd, hidden_nodes), nn.ReLU(), nn.Linear(hidden_nodes, block_size),) # this can be a linear layer but added non-linearity with some hidden_nodes for finer control of param number\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "\n",
    "        # k = self.key(x)   # (B,T,hs)\n",
    "        # q = self.query(x) # (B,T,hs)\n",
    "        # wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "\n",
    "        wei = self.wr(x)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.267853 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 6.2752, val loss 6.2847\n",
      "step 500: train loss 0.3153, val loss 6.7114\n",
      "step 1000: train loss 0.1748, val loss 7.5776\n",
      "step 1500: train loss 0.1514, val loss 8.0265\n",
      "step 2000: train loss 0.1390, val loss 8.3342\n",
      "step 2500: train loss 0.1358, val loss 8.7169\n",
      "step 3000: train loss 0.1203, val loss 8.6746\n",
      "step 3500: train loss 0.1174, val loss 9.0160\n",
      "step 4000: train loss 0.1193, val loss 9.0930\n",
      "step 4500: train loss 0.1182, val loss 9.3440\n",
      "step 4999: train loss 0.1135, val loss 9.4117\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, block_size), dtype=torch.long, device=device)\n",
    "context += xb[0, :]\n",
    "gen_text = decode(m.generate(context, max_new_tokens=3000)[0].tolist())\n",
    "# gen_text\n",
    "print(gen_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
